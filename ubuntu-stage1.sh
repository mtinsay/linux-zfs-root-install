#!/bin/bash

# ZFS Root Installation Scripts - Stage 1 (v0.1)
# Copyright (C) 2025 Michael C. Tinsay
# 
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program. If not, see <https://www.gnu.org/licenses/>.
#
# Based on OpenZFS documentation
# WARNING: This script will destroy data on the target disk!
#
# ⚠️  AI-GENERATED CODE DISCLAIMER ⚠️
# This script was entirely generated by AI without direct human editing.
# No comprehensive human testing or code review has been performed.
# Thoroughly test in isolated environments and have qualified personnel
# review before using in any production or important system.
# Use at your own risk - AI-generated code may contain subtle bugs.

# Enhanced bash options for better error handling
set -Euo pipefail

# Error handling with line number reporting
error_exit() {
    local exit_code=$?
    if [[ $exit_code -eq 99 ]]; then
        echo -e "${YELLOW}DEBUG BREAK: Debug break triggered in stage 2 or stage 3${NC}" >&2
        echo -e "${YELLOW}Installation stopped for debugging purposes${NC}" >&2
        exit 99
    else
        echo -e "${RED}ERROR: Script failed at line $1${NC}" >&2
        echo -e "${RED}Command: $2${NC}" >&2
        exit 1
    fi
}
trap 'error_exit ${LINENO} "$BASH_COMMAND"' ERR

# Load configuration
SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
CONFIG_FILE="$SCRIPT_DIR/ubuntu-config.sh"

if [[ ! -f "$CONFIG_FILE" ]]; then
    echo "Error: Configuration file $CONFIG_FILE not found!"
    echo "Please ensure ubuntu-config.sh exists in the same directory as this script."
    exit 1
fi

source "$CONFIG_FILE"

# Colors are now loaded from ubuntu-config.sh

log() {
    echo -e "${GREEN}[INFO]${NC} $1"
}

warn() {
    echo -e "${YELLOW}[WARN]${NC} $1"
}

debug_break() {
    if [[ "${DEBUG:-false}" == "true" ]]; then
        warn "DEBUG BREAK: $1"
        warn "Exiting with error status for debugging..."
        exit 99  # Special debug exit code
    fi
}

error() {
    echo -e "${RED}[ERROR]${NC} $1"
    exit 1
}

check_root() {
    if [[ $EUID -ne 0 ]]; then
        error "This script must be run as root"
    fi
}

check_uefi_boot() {
    log "Checking UEFI boot mode..."
    
    if [[ ! -d "/sys/firmware/efi" ]]; then
        error "System was not booted in UEFI mode!"
        error "This script requires UEFI boot mode for proper installation."
        error "Please:"
        error "  1. Enable UEFI mode in your BIOS/firmware settings"
        error "  2. Boot from the installation media in UEFI mode"
        error "  3. Verify /sys/firmware/efi directory exists"
        exit 1
    fi
    
    log "UEFI boot mode confirmed."
}

detect_existing_pools() {
    log "Detecting existing ZFS pools..."
    
    # Import any available pools for detection (non-destructive)
    zpool import -a -N 2>/dev/null || true
    
    # Check for existing root pool
    if [[ "$REUSE_ROOT_POOL" == "true" ]]; then
        # First check if pool is already imported
        if zpool list "$POOL_NAME" >/dev/null 2>&1; then
            log "Found existing imported root pool: $POOL_NAME"
            EXISTING_ROOT_POOL_FOUND="true"
        else
            # Check if pool exists but is not imported
            if zpool import -d /dev 2>/dev/null | grep -q "pool: $POOL_NAME"; then
                log "Found existing unimported root pool: $POOL_NAME"
                EXISTING_ROOT_POOL_FOUND="true"
            else
                log "REUSE_ROOT_POOL=true but pool '$POOL_NAME' not found - will create new pool"
                EXISTING_ROOT_POOL_FOUND="false"
            fi
        fi
        
        # Validate existing root dataset if specified and reusing pool
        if [[ -n "$EXISTING_ROOT_DATASET" && "$EXISTING_ROOT_POOL_FOUND" == "true" ]]; then
            # Ensure the pool containing the dataset is imported
            local dataset_pool="${EXISTING_ROOT_DATASET%%/*}"
            if ! zpool list "$dataset_pool" >/dev/null 2>&1; then
                log "Importing pool for dataset validation: $dataset_pool"
                zpool import -N "$dataset_pool" 2>/dev/null || true
            fi
            
            if ! zfs list "$EXISTING_ROOT_DATASET" >/dev/null 2>&1; then
                error "Specified EXISTING_ROOT_DATASET '$EXISTING_ROOT_DATASET' not found"
            fi
            log "Found existing root dataset: $EXISTING_ROOT_DATASET"
        fi
    else
        # REUSE_ROOT_POOL=false: Destroy existing pool if it exists
        EXISTING_ROOT_POOL_FOUND="false"
        
        # Check if pool exists and destroy it
        local pool_exists="false"
        local pools_to_destroy=()
        
        # Check for pool with matching name
        if zpool list "$POOL_NAME" >/dev/null 2>&1; then
            log "Found existing imported pool: $POOL_NAME"
            pool_exists="true"
            pools_to_destroy+=("$POOL_NAME")
        elif zpool import -d /dev 2>/dev/null | grep -q "pool: $POOL_NAME"; then
            log "Found existing unimported pool: $POOL_NAME"
            pool_exists="true"
            pools_to_destroy+=("$POOL_NAME")
        fi
        
        # Also check for any pools using the ROOT_PARTITION
        if [[ -n "$ROOT_PARTITION" ]]; then
            log "Checking for pools using ROOT_PARTITION: $ROOT_PARTITION"
            
            # Get all pools and check their devices
            while IFS= read -r existing_pool; do
                if [[ -n "$existing_pool" && "$existing_pool" != "$POOL_NAME" ]]; then
                    # Get devices used by this pool
                    local pool_devices
                    pool_devices=$(zpool list -v -H -P "$existing_pool" 2>/dev/null | awk '{print $1}' | grep "^/dev/" | head -10)
                    
                    # Check if any device matches our ROOT_PARTITION
                    while IFS= read -r device; do
                        if [[ -n "$device" && "$device" == "$ROOT_PARTITION" ]]; then
                            log "Found pool '$existing_pool' using ROOT_PARTITION: $ROOT_PARTITION"
                            pools_to_destroy+=("$existing_pool")
                            pool_exists="true"
                            break
                        fi
                    done <<< "$pool_devices"
                fi
            done <<< "$(zpool list -H -o name 2>/dev/null || true)"
        fi
        
        # Destroy all identified pools
        for pool_to_destroy in "${pools_to_destroy[@]}"; do
            log "REUSE_ROOT_POOL=false: Destroying existing pool '$pool_to_destroy'"
            
            # Import pool if not already imported (needed for destruction)
            if ! zpool list "$pool_to_destroy" >/dev/null 2>&1; then
                log "Importing pool for destruction: $pool_to_destroy"
                zpool import -f "$pool_to_destroy" 2>/dev/null || true
            fi
            
            # Destroy the pool
            if zpool list "$pool_to_destroy" >/dev/null 2>&1; then
                log "Destroying pool: $pool_to_destroy"
                zpool destroy -f "$pool_to_destroy" || {
                    warn "Failed to destroy pool $pool_to_destroy, attempting export..."
                    zpool export -f "$pool_to_destroy" 2>/dev/null || true
                }
            fi
        done
        
        # Ignore EXISTING_ROOT_DATASET when not reusing pool
        if [[ -n "$EXISTING_ROOT_DATASET" ]]; then
            log "REUSE_ROOT_POOL=false: Ignoring EXISTING_ROOT_DATASET setting"
        fi
    fi
}

validate_required_config() {
    log "Validating configuration variables..."
    
    local missing_vars=()
    
    # Required variables for all modes
    local required_vars=(
        "PARTITION_MODE"
        "HOSTNAME"
        "DEBOOTSTRAP_SUITE"
        "INSTALL_ROOT"
        "AUTO_RUN_STAGE2"
        "DEBUG"
        "CACHE_DIR"
        "POOL_NAME"
        "ROOT_DATASET_NAME"
        "REUSE_ROOT_POOL"
        "NETWORK_INTERFACE"
        "TIMEZONE"
        "LOCALE"
        "ZFS_MOUNT_METHOD"
        "RED"
        "GREEN"
        "YELLOW"
        "NC"
    )
    
    # SWAP_SIZE is optional - can be empty to disable swap
    # Check if SWAP_SIZE variable exists (but allow it to be empty)
    if ! declare -p SWAP_SIZE &>/dev/null; then
        missing_vars+=("SWAP_SIZE")
    fi
    
    # Check required variables
    for var in "${required_vars[@]}"; do
        if [[ -z "${!var}" ]]; then
            missing_vars+=("$var")
        fi
    done
    
    # Mode-specific required variables
    if [[ "$PARTITION_MODE" == "auto" ]]; then
        local auto_vars=("DISK" "EFI_SIZE" "BOOT_SIZE")
        for var in "${auto_vars[@]}"; do
            if [[ -z "${!var}" ]]; then
                missing_vars+=("$var")
            fi
        done
    elif [[ "$PARTITION_MODE" == "manual" ]]; then
        local manual_vars=("EFI_PARTITION" "BOOT_PARTITION" "ROOT_PARTITION")
        for var in "${manual_vars[@]}"; do
            if [[ -z "${!var}" ]]; then
                missing_vars+=("$var")
            fi
        done
    fi
    
    # Check if ZFS_DATASETS array exists
    if [[ -z "${ZFS_DATASETS[*]}" ]]; then
        missing_vars+=("ZFS_DATASETS")
    fi
    
    # Check if ADDITIONAL_PACKAGES array exists (allow empty array)
    if ! declare -p ADDITIONAL_PACKAGES &>/dev/null; then
        missing_vars+=("ADDITIONAL_PACKAGES")
    fi
    
    # Check if USERS array exists
    if [[ -z "${USERS[*]}" ]]; then
        missing_vars+=("USERS")
    fi
    
    # Report missing variables
    if [[ ${#missing_vars[@]} -gt 0 ]]; then
        error "Missing required configuration variables in ubuntu-config.sh:"
        for var in "${missing_vars[@]}"; do
            error "  - $var"
        done
        error ""
        error "Please check your ubuntu-config.sh file and ensure all required variables are defined."
        error "You may have accidentally deleted or commented out some configuration entries."
        exit 1
    fi
    
    log "Configuration validation passed."
}

check_configuration() {
    local skip_confirmation="$1"
    
    if [[ "$PARTITION_MODE" == "auto" ]]; then
        if [[ ! -b "$DISK" ]]; then
            error "Disk $DISK does not exist"
        fi
        
        warn "Auto partitioning mode: This will DESTROY all data on $DISK"
        
        if [[ "$skip_confirmation" == "true" ]]; then
            log "Skipping confirmation prompt (-y parameter provided)"
        else
            read -p "Are you sure you want to continue? (yes/no): " confirm
            if [[ "$confirm" != "yes" ]]; then
                error "Installation cancelled"
            fi
        fi
    elif [[ "$PARTITION_MODE" == "manual" ]]; then
        log "Manual partitioning mode: Using existing partitions"
        
        # Check required partitions
        if [[ ! -b "$EFI_PARTITION" ]]; then
            error "EFI partition $EFI_PARTITION does not exist"
        fi
        if [[ ! -b "$BOOT_PARTITION" ]]; then
            error "Boot partition $BOOT_PARTITION does not exist"
        fi
        if [[ ! -b "$ROOT_PARTITION" ]]; then
            error "Root partition $ROOT_PARTITION does not exist"
        fi
        
        # Check optional swap partition
        if [[ -n "$SWAP_PARTITION" && ! -b "$SWAP_PARTITION" ]]; then
            error "Swap partition $SWAP_PARTITION does not exist"
        fi
        
        # Validate EFI and Boot partition compatibility
        validate_boot_efi_compatibility
        
        log "Using partitions:"
        log "  EFI: $EFI_PARTITION"
        log "  Boot: $BOOT_PARTITION"
        log "  Root: $ROOT_PARTITION"
        if [[ -n "$SWAP_PARTITION" ]]; then
            log "  Swap: $SWAP_PARTITION"
        fi
    else
        error "Invalid PARTITION_MODE: $PARTITION_MODE (must be 'auto' or 'manual')"
    fi
}

validate_boot_efi_compatibility() {
    log "Validating EFI and Boot partition compatibility..."
    
    local efi_drive=$(echo "$EFI_PARTITION" | sed 's/[0-9]*$//')
    
    # Check if boot partition is an mdadm RAID device
    if [[ "$BOOT_PARTITION" =~ ^/dev/md[0-9]+$ ]]; then
        log "Boot partition is mdadm RAID device: $BOOT_PARTITION"
        
        # Verify RAID device exists and is active
        if [[ ! -b "$BOOT_PARTITION" ]]; then
            error "RAID device $BOOT_PARTITION does not exist"
        fi
        
        if ! mdadm --detail "$BOOT_PARTITION" >/dev/null 2>&1; then
            error "RAID device $BOOT_PARTITION is not active or accessible"
        fi
        
        # Get RAID level and validate it's RAID 1
        local raid_level
        raid_level=$(mdadm --detail "$BOOT_PARTITION" | grep "Raid Level" | awk '{print $4}')
        
        if [[ "$raid_level" != "raid1" ]]; then
            error "Boot partition RAID device must be RAID 1 (mirror), found: $raid_level"
            error "GRUB requires RAID 1 for boot partition support"
        fi
        
        log "Verified RAID 1 configuration for boot partition"
        
        # Get active devices in the RAID array
        local raid_devices
        raid_devices=$(mdadm --detail "$BOOT_PARTITION" | grep -E "^\s+[0-9]+\s+[0-9]+\s+[0-9]+\s+[0-9]+\s+active sync" | awk '{print $7}')
        
        if [[ -z "$raid_devices" ]]; then
            error "No active devices found in RAID array $BOOT_PARTITION"
        fi
        
        log "Active RAID devices: $raid_devices"
        
        # Check if any RAID device is on the same drive as EFI partition
        local efi_compatible="false"
        for device in $raid_devices; do
            local device_drive=$(echo "$device" | sed 's/[0-9]*$//')
            log "Checking RAID device $device (drive: $device_drive) against EFI drive: $efi_drive"
            
            if [[ "$device_drive" == "$efi_drive" ]]; then
                log "Found RAID device $device on same drive as EFI partition ($efi_drive)"
                efi_compatible="true"
                break
            fi
        done
        
        if [[ "$efi_compatible" != "true" ]]; then
            error "RAID 1 boot partition requires at least one active device on the same drive as EFI partition"
            error "EFI partition: $EFI_PARTITION (drive: $efi_drive)"
            error "RAID devices: $raid_devices"
            error "None of the RAID devices are on drive $efi_drive"
            error "This is required for GRUB installation compatibility"
        fi
        
        log "RAID 1 boot partition validation passed - compatible with EFI partition"
        
    else
        # Standard partition validation
        local boot_drive=$(echo "$BOOT_PARTITION" | sed 's/[0-9]*$//')
        
        if [[ "$efi_drive" != "$boot_drive" ]]; then
            error "EFI partition ($EFI_PARTITION) and Boot partition ($BOOT_PARTITION) must be on the same drive"
            error "EFI drive: $efi_drive, Boot drive: $boot_drive"
            error "This is required for proper GRUB installation"
            error ""
            error "Alternative: Use mdadm RAID 1 for boot partition with one device on the EFI drive"
        fi
        
        log "Standard boot partition validation passed - same drive as EFI partition"
    fi
}

install_zfs() {
    log "Installing ZFS packages..."
    apt update || true
    apt install -y debootstrap zfsutils-linux zfs-initramfs
}

unmount_all_partitions_on_disk() {
    local target_disk="$1"
    log "Unmounting all partitions on $target_disk..."
    
    # Get all partitions on the target disk
    local partitions=$(lsblk -ln -o NAME "$target_disk" | grep -v "^$(basename "$target_disk")$" | sed "s|^|/dev/|")
    
    if [[ -z "$partitions" ]]; then
        log "No partitions found on $target_disk"
        return 0
    fi
    
    # First, disable any swap partitions on the target disk
    log "Disabling swap partitions on $target_disk..."
    for partition in $partitions; do
        if swapon --show=NAME --noheadings | grep -q "^$partition$"; then
            log "Disabling swap on $partition..."
            swapoff "$partition" || warn "Failed to disable swap on $partition"
        fi
    done
    
    # Unmount all mounted partitions on the target disk
    log "Unmounting mounted partitions on $target_disk..."
    for partition in $partitions; do
        # Check if partition is mounted
        if mount | grep -q "^$partition "; then
            local mount_point=$(mount | grep "^$partition " | awk '{print $3}')
            log "Unmounting $partition from $mount_point..."
            umount "$partition" || warn "Failed to unmount $partition"
        fi
    done
    
    # Force unmount any remaining mounts (lazy unmount as last resort)
    for partition in $partitions; do
        if mount | grep -q "^$partition "; then
            warn "Force unmounting $partition (lazy unmount)..."
            umount -l "$partition" || warn "Failed to force unmount $partition"
        fi
    done
    
    log "Finished unmounting partitions on $target_disk"
}

destroy_existing_zfs_pools_on_disk() {
    local target_disk="$1"
    log "Checking for existing ZFS pools on $target_disk..."
    
    # Get all pools and check which ones use partitions from the target disk
    local pools_to_destroy=()
    
    # List all pools and check their devices
    while IFS= read -r pool_name; do
        if [[ -n "$pool_name" ]]; then
            # Get devices used by this pool
            local pool_devices=$(zpool list -v -H -P "$pool_name" 2>/dev/null | awk '{print $1}' | grep "^/dev/" | head -10)
            
            # Check if any device belongs to our target disk
            while IFS= read -r device; do
                if [[ -n "$device" && "$device" == ${target_disk}* ]]; then
                    pools_to_destroy+=("$pool_name")
                    break
                fi
            done <<< "$pool_devices"
        fi
    done <<< "$(zpool list -H -o name 2>/dev/null || true)"
    
    # Unmount and destroy pools that use the target disk
    for pool in "${pools_to_destroy[@]}"; do
        log "Preparing to destroy existing ZFS pool: $pool (uses $target_disk)"
        
        # First, unmount all datasets in this pool
        log "Unmounting all datasets in pool $pool..."
        
        # Get all mounted datasets for this pool (sorted by mount depth, deepest first)
        local mounted_datasets=$(zfs list -H -o name,mountpoint -t filesystem -r "$pool" 2>/dev/null | \
            awk '$2 != "none" && $2 != "legacy" && $2 != "-" {print length($2), $1, $2}' | \
            sort -nr | awk '{print $2, $3}' || true)
        
        if [[ -n "$mounted_datasets" ]]; then
            while IFS=' ' read -r dataset mountpoint; do
                if [[ -n "$dataset" && -n "$mountpoint" ]]; then
                    log "Unmounting ZFS dataset: $dataset (mounted at $mountpoint)"
                    zfs unmount "$dataset" 2>/dev/null || {
                        warn "Failed to unmount $dataset, forcing unmount..."
                        zfs unmount -f "$dataset" 2>/dev/null || true
                    }
                fi
            done <<< "$mounted_datasets"
        fi
        
        # Also check for any legacy-mounted datasets and unmount them
        log "Checking for legacy-mounted datasets in pool $pool..."
        local legacy_datasets=$(zfs list -H -o name,mountpoint -t filesystem -r "$pool" 2>/dev/null | \
            awk '$2 == "legacy" {print $1}' || true)
        
        if [[ -n "$legacy_datasets" ]]; then
            while IFS= read -r dataset; do
                if [[ -n "$dataset" ]]; then
                    # Find mount points for legacy datasets by checking /proc/mounts
                    local legacy_mounts=$(grep " zfs " /proc/mounts | grep "$dataset " | awk '{print $2}' || true)
                    if [[ -n "$legacy_mounts" ]]; then
                        while IFS= read -r mount_point; do
                            if [[ -n "$mount_point" ]]; then
                                log "Unmounting legacy ZFS dataset: $dataset (mounted at $mount_point)"
                                umount "$mount_point" 2>/dev/null || {
                                    warn "Failed to unmount $mount_point, forcing unmount..."
                                    umount -f "$mount_point" 2>/dev/null || umount -l "$mount_point" 2>/dev/null || true
                                }
                            fi
                        done <<< "$legacy_mounts"
                    fi
                fi
            done <<< "$legacy_datasets"
        fi
        
        # Unmount any remaining subdirectories that might be mounted under pool datasets
        log "Checking for any remaining mounted subdirectories related to pool $pool..."
        local pool_related_mounts=$(mount | grep -E "(^$pool/|/$pool/)" | awk '{print $3}' | sort -r || true)
        if [[ -n "$pool_related_mounts" ]]; then
            while IFS= read -r mount_point; do
                if [[ -n "$mount_point" ]]; then
                    log "Unmounting pool-related mount: $mount_point"
                    umount "$mount_point" 2>/dev/null || {
                        warn "Failed to unmount $mount_point, forcing unmount..."
                        umount -f "$mount_point" 2>/dev/null || umount -l "$mount_point" 2>/dev/null || true
                    }
                fi
            done <<< "$pool_related_mounts"
        fi
        
        # Now attempt to destroy the pool
        log "Destroying ZFS pool: $pool"
        zpool destroy -f "$pool" 2>/dev/null || {
            warn "Failed to destroy pool $pool, attempting export..."
            zpool export -f "$pool" 2>/dev/null || true
        }
    done
    
    if [[ ${#pools_to_destroy[@]} -eq 0 ]]; then
        log "No existing ZFS pools found on $target_disk"
    fi
}

partition_disk() {
    if [[ "$PARTITION_MODE" == "auto" ]]; then
        log "Auto partitioning disk $DISK..."
        
        # Unmount all partitions on the target disk first
        unmount_all_partitions_on_disk "$DISK"
        
        # Destroy any existing ZFS pools on the target drive
        destroy_existing_zfs_pools_on_disk "$DISK"
        
        # Remove all filesystem signatures from the disk
        log "Removing all filesystem signatures from $DISK..."
        wipefs --all --force $DISK
        
        # Create fresh GPT partition table using sfdisk
        log "Creating fresh GPT partition table on $DISK..."
        # NOTE: BIOS boot partition is commented out for now since GRUB installation
        # is currently fixed to UEFI mode only. Uncomment the BIOS boot partition
        # lines below for non-UEFI installations, but additional GRUB configuration
        # changes will be needed in stage2 install_grub() function.
        
        if [[ "$SWAP_SIZE" != "0" && -n "$SWAP_SIZE" ]]; then
            log "Creating partitions with EFI, boot, swap, and root partitions (UEFI only)"
            sfdisk $DISK << EOF
label: gpt
unit: sectors

# Uncomment for BIOS boot support (requires GRUB install changes):
# start=2048, size=1M, type=21686148-6449-6E6F-744E-656564454649
start=2048, size=+$EFI_SIZE, type=C12A7328-F81F-11D2-BA4B-00A0C93EC93B
start=, size=+$BOOT_SIZE, type=0FC63DAF-8483-4772-8E79-3D69D8477DE4
start=, size=+$SWAP_SIZE, type=0657FD6D-A4AB-43C4-84E5-0933C84B4F4F
start=, size=, type=0FC63DAF-8483-4772-8E79-3D69D8477DE4
EOF
            # Set partition variables for use in other functions
            # BIOS_BOOT_PARTITION="${DISK}1"  # Uncomment when BIOS boot is enabled
            EFI_PARTITION="${DISK}1"
            BOOT_PARTITION="${DISK}2"
            SWAP_PARTITION="${DISK}3"
            ROOT_PARTITION="${DISK}4"
        else
            log "SWAP_SIZE is 0 or empty - swap partition disabled"
            log "Creating partitions with EFI, boot, and root partitions (UEFI only)"
            sfdisk $DISK << EOF
label: gpt
unit: sectors

# Uncomment for BIOS boot support (requires GRUB install changes):
# start=2048, size=1M, type=21686148-6449-6E6F-744E-656564454649
start=2048, size=+$EFI_SIZE, type=C12A7328-F81F-11D2-BA4B-00A0C93EC93B
start=, size=+$BOOT_SIZE, type=0FC63DAF-8483-4772-8E79-3D69D8477DE4
start=, size=, type=0FC63DAF-8483-4772-8E79-3D69D8477DE4
EOF
            # Set partition variables for use in other functions
            # BIOS_BOOT_PARTITION="${DISK}1"  # Uncomment when BIOS boot is enabled
            EFI_PARTITION="${DISK}1"
            BOOT_PARTITION="${DISK}2"
            ROOT_PARTITION="${DISK}3"
            SWAP_PARTITION=""
        fi
        
        # Inform kernel of partition changes
        partprobe $DISK
        sleep 2
    else
        log "Using existing partitions (manual mode)"
        # Partition variables are already set from config
    fi
}

create_zfs_pools() {
    # Create or reuse root pool only (boot will use ext4)
    if [[ "$EXISTING_ROOT_POOL_FOUND" == "true" ]]; then
        # Reuse existing pool
        log "Reusing existing root pool: $POOL_NAME"
        
        # Check if pool is already imported
        if ! zpool list "$POOL_NAME" >/dev/null 2>&1; then
            log "Importing existing pool: $POOL_NAME"
            if ! zpool import -f -R $INSTALL_ROOT "$POOL_NAME"; then
                error "Failed to import existing pool: $POOL_NAME"
            fi
        else
            log "Pool $POOL_NAME already imported, setting altroot"
            zpool set altroot=$INSTALL_ROOT $POOL_NAME
        fi
        
        # Set cache file for the reused pool
        log "Setting ZFS cache file for reused pool $POOL_NAME..."
        zpool set cachefile=/etc/zfs/zpool.cache $POOL_NAME
    else
        # Create new pool (existing pool was already destroyed if needed)
        log "Creating new root pool: $POOL_NAME"
        
        # Final safety check - ensure no pool exists with this name
        if zpool list "$POOL_NAME" >/dev/null 2>&1; then
            error "Pool $POOL_NAME still exists after cleanup - cannot create new pool"
            error "This indicates a problem with pool destruction"
            exit 1
        fi
        
        zpool create -f \
            -o ashift=12 \
            -o autotrim=on \
            -O acltype=posixacl \
            -O canmount=off \
            -O compression=zstd \
            -O devices=off \
            -O dnodesize=auto \
            -O normalization=formD \
            -O relatime=on \
            -O sync=standard \
            -O xattr=sa \
            -O mountpoint=/ \
            -R $INSTALL_ROOT \
            $POOL_NAME $ROOT_PARTITION
        
        # Set cache file for the new pool
        log "Setting ZFS cache file for pool $POOL_NAME..."
        zpool set cachefile=/etc/zfs/zpool.cache $POOL_NAME
        
        # Export and re-import pool to ensure clean state
        log "Exporting and re-importing pool $POOL_NAME for clean state..."
        zpool export $POOL_NAME
        zpool import -f -R $INSTALL_ROOT $POOL_NAME
    fi
}

create_datasets() {
    log "Creating or configuring ZFS datasets..."
    
    # Handle root dataset
    if [[ "$PARTITION_MODE" == "auto" ]]; then
        # In auto mode, always create new root dataset (existing pools were destroyed)
        log "Auto mode: Creating new root dataset"
        zfs create -o mountpoint=/ -o canmount=on -o devices=on $POOL_NAME/$ROOT_DATASET_NAME
        # ZFS automatically mounts with canmount=on
    elif [[ -n "$EXISTING_ROOT_DATASET" ]]; then
        # Manual mode: Check if the specified root dataset exists
        if zfs list "$EXISTING_ROOT_DATASET" >/dev/null 2>&1; then
            log "Using existing root dataset: $EXISTING_ROOT_DATASET"
            zfs set mountpoint=/ $EXISTING_ROOT_DATASET
            zfs set canmount=on $EXISTING_ROOT_DATASET
            # Mount existing dataset (may not be mounted after pool import)
            zfs mount "$EXISTING_ROOT_DATASET"
        else
            log "EXISTING_ROOT_DATASET specified but doesn't exist - creating: $EXISTING_ROOT_DATASET"
            zfs create -o mountpoint=/ -o canmount=on -o devices=on "$EXISTING_ROOT_DATASET"
            # ZFS automatically mounts with canmount=on
        fi
    else
        # Manual mode: Check if reusing existing pool with default root dataset
        if [[ "$REUSE_ROOT_POOL" == "true" && "$EXISTING_ROOT_POOL_FOUND" == "true" ]]; then
            # Check if default root dataset exists in reused pool
            if zfs list "$POOL_NAME/$ROOT_DATASET_NAME" >/dev/null 2>&1; then
                log "Using existing root dataset from reused pool: $POOL_NAME/$ROOT_DATASET_NAME"
                zfs set mountpoint=/ "$POOL_NAME/$ROOT_DATASET_NAME"
                zfs set canmount=on "$POOL_NAME/$ROOT_DATASET_NAME"
                # Mount existing dataset (may not be mounted after pool import)
                zfs mount "$POOL_NAME/$ROOT_DATASET_NAME"
            else
                log "Creating new root dataset in reused pool: $POOL_NAME/$ROOT_DATASET_NAME"
                zfs create -o mountpoint=/ -o canmount=on -o devices=on $POOL_NAME/$ROOT_DATASET_NAME
                # ZFS automatically mounts with canmount=on
            fi
        else
            # Manual mode: Create new root dataset (this will be the filesystem root)
            zfs create -o mountpoint=/ -o canmount=on -o devices=on $POOL_NAME/$ROOT_DATASET_NAME
            # ZFS automatically mounts with canmount=on
        fi
    fi
    
    # Ensure essential directories exist for subsequent operations
    log "Ensuring essential directories exist..."
    mkdir -p $INSTALL_ROOT/etc
    
    # Boot partition will use ext4, no ZFS datasets needed
    
    # Create additional datasets only if they don't exist (for reused pools)
    create_dataset_if_not_exists() {
        local dataset="$1"
        shift
        if ! zfs list "$dataset" >/dev/null 2>&1; then
            zfs create "$@" "$dataset"
        fi
    }
    
    # Parse and create dataset from config format: "name:mountpoint:options"
    create_dataset_from_config() {
        local config="$1"
        local dataset_name mountpoint options
        
        IFS=':' read -r dataset_name mountpoint options <<< "$config"
        
        local full_dataset="$POOL_NAME/$ROOT_DATASET_NAME/$dataset_name"
        local create_args=()
        
        # Force mountpoint=legacy and canmount=noauto for all datasets
        create_args+=("-o" "mountpoint=legacy")
        create_args+=("-o" "canmount=noauto")
        
        # Add additional options if specified, filtering out any mountpoint options
        if [[ -n "$options" ]]; then
            IFS=',' read -ra opts <<< "$options"
            for opt in "${opts[@]}"; do
                # Skip any mountpoint options since we force mountpoint=legacy
                if [[ "$opt" != mountpoint=* ]]; then
                    create_args+=("-o" "$opt")
                fi
            done
        fi
        
        create_dataset_if_not_exists "$full_dataset" "${create_args[@]}"
    }
    
    # Create all configured datasets
    log "Creating ZFS datasets..."
    for dataset_config in "${ZFS_DATASETS[@]}"; do
        # Skip empty entries and comments
        if [[ -n "$dataset_config" && ! "$dataset_config" =~ ^[[:space:]]*# ]]; then
            create_dataset_from_config "$dataset_config"
        fi
    done
    
    # Mount all created datasets
    log "Mounting ZFS datasets..."
    for dataset_config in "${ZFS_DATASETS[@]}"; do
        # Skip empty entries and comments
        if [[ -n "$dataset_config" && ! "$dataset_config" =~ ^[[:space:]]*# ]]; then
            local dataset_name mountpoint options
            IFS=':' read -r dataset_name mountpoint options <<< "$dataset_config"
            
            local full_dataset="$POOL_NAME/$ROOT_DATASET_NAME/$dataset_name"
            local mount_path="$INSTALL_ROOT$mountpoint"
            
            # Create mount directory if it doesn't exist
            mkdir -p "$mount_path"
            
            # Mount the dataset
            log "Mounting $full_dataset to $mount_path"
            mount -v -t zfs "$full_dataset" "$mount_path"
        fi
    done
    
    # Set permissions for var/tmp if it was created
    if [[ -d "$INSTALL_ROOT/var/tmp" ]]; then
        chmod 1777 $INSTALL_ROOT/var/tmp
    fi
}

configure_dataset_mounting() {
    log "Configuring dataset mounting method: $ZFS_MOUNT_METHOD"
    
    if [[ "$ZFS_MOUNT_METHOD" == "fstab" ]]; then
        generate_zfs_fstab_entries
    elif [[ "$ZFS_MOUNT_METHOD" == "zfs" ]]; then
        configure_zfs_native_mounting
    else
        error "Invalid ZFS_MOUNT_METHOD: $ZFS_MOUNT_METHOD (must be 'fstab' or 'zfs')"
    fi
}

generate_zfs_fstab_entries() {
    log "Generating comprehensive fstab entries..."
    
    # Create fstab header
    cat > $INSTALL_ROOT/etc/fstab << 'EOF'
# /etc/fstab: static file system information.
#
# Use 'blkid' to print the universally unique identifier for a
# device; this may be used with UUID= as a more robust way to name devices
# that works even if disks are added and removed. See fstab(5).
#
# <file system> <mount point>   <type>  <options>       <dump>  <pass>

EOF
    
    # Root ZFS dataset uses mountpoint=/ so it's mounted automatically by ZFS
    # No fstab entry needed for root dataset
    
    # Get UUIDs for non-ZFS partitions
    local boot_uuid efi_uuid swap_uuid
    if [[ -n "$BOOT_PARTITION" ]]; then
        boot_uuid=$(blkid -s UUID -o value "$BOOT_PARTITION" 2>/dev/null || echo "")
    fi
    if [[ -n "$EFI_PARTITION" ]]; then
        efi_uuid=$(blkid -s UUID -o value "$EFI_PARTITION" 2>/dev/null || echo "")
    fi
    if [[ -n "$SWAP_PARTITION" ]]; then
        swap_uuid=$(blkid -s UUID -o value "$SWAP_PARTITION" 2>/dev/null || echo "")
    fi
    
    # Add non-ZFS partitions using /dev/disk/by-uuid/ references (in hierarchical order)
    if [[ -n "$boot_uuid" ]]; then
        echo "/dev/disk/by-uuid/$boot_uuid /boot ext4 defaults 0 2" >> $INSTALL_ROOT/etc/fstab
    elif [[ -n "$BOOT_PARTITION" ]]; then
        echo "$BOOT_PARTITION /boot ext4 defaults 0 2" >> $INSTALL_ROOT/etc/fstab
    fi
    
    if [[ -n "$efi_uuid" ]]; then
        echo "/dev/disk/by-uuid/$efi_uuid /boot/efi vfat umask=0077 0 1" >> $INSTALL_ROOT/etc/fstab
    elif [[ -n "$EFI_PARTITION" ]]; then
        echo "$EFI_PARTITION /boot/efi vfat umask=0077 0 1" >> $INSTALL_ROOT/etc/fstab
    fi
    
    if [[ -n "$swap_uuid" ]]; then
        echo "/dev/disk/by-uuid/$swap_uuid none swap sw 0 0" >> $INSTALL_ROOT/etc/fstab
    elif [[ -n "$SWAP_PARTITION" ]]; then
        echo "$SWAP_PARTITION none swap sw 0 0" >> $INSTALL_ROOT/etc/fstab
    fi
    
    # Add ZFS datasets in hierarchical order based on ZFS_DATASETS configuration
    local sorted_datasets=()
    
    # Parse ZFS_DATASETS and sort by mount depth
    for dataset_config in "${ZFS_DATASETS[@]}"; do
        # Skip empty entries and comments
        if [[ -n "$dataset_config" && ! "$dataset_config" =~ ^[[:space:]]*# ]]; then
            local dataset_name mountpoint options
            IFS=':' read -r dataset_name mountpoint options <<< "$dataset_config"
            
            if [[ -n "$mountpoint" ]]; then
                # Count depth by number of slashes
                local depth=$(echo "$mountpoint" | tr -cd '/' | wc -c)
                sorted_datasets+=("$depth:$dataset_name:$mountpoint")
            fi
        fi
    done
    
    # Sort by depth (hierarchical order)
    IFS=$'\n' sorted_datasets=($(sort -n <<< "${sorted_datasets[*]}"))
    
    # Add ZFS dataset entries to fstab
    for entry in "${sorted_datasets[@]}"; do
        local depth dataset_name mountpoint
        IFS=':' read -r depth dataset_name mountpoint <<< "$entry"
        
        local full_dataset="$POOL_NAME/$ROOT_DATASET_NAME/$dataset_name"
        echo "$full_dataset $mountpoint zfs defaults 0 0" >> $INSTALL_ROOT/etc/fstab
    done
    
    log "Generated fstab with hierarchical ordering and UUIDs for non-ZFS devices"
}

configure_zfs_native_mounting() {
    log "Configuring ZFS native mounting..."
    
    # For ZFS native mounting, we need to ensure datasets can be mounted
    # but we still want them to be noauto so they don't mount during pool import
    # They will be mounted by 'zfs mount -a' service
    
    # Create a systemd service to mount ZFS datasets after pool import
    cat > $INSTALL_ROOT/etc/systemd/system/zfs-mount-datasets.service << 'EOF'
[Unit]
Description=Mount ZFS datasets
After=zfs-import.target
Wants=zfs-import.target
DefaultDependencies=no

[Service]
Type=oneshot
RemainAfterExit=yes
ExecStart=/sbin/zfs mount -a
ExecStop=/sbin/zfs unmount -a

[Install]
WantedBy=multi-user.target
EOF
}

format_efi() {
    log "Configuring EFI partition..."
    
    # Check if EFI partition is already formatted
    if blkid $EFI_PARTITION | grep -q "TYPE=\"vfat\""; then
        log "EFI partition is already formatted as FAT32"
        
        # Check if it contains EFI files (indicating it's in use)
        mkdir -p /tmp/efi_check
        if mount -v $EFI_PARTITION /tmp/efi_check 2>/dev/null; then
            if [[ -d "/tmp/efi_check/EFI" ]] && [[ -n "$(ls -A /tmp/efi_check/EFI 2>/dev/null)" ]]; then
                log "EFI partition contains existing EFI files - preserving existing format"
                umount /tmp/efi_check
                rmdir /tmp/efi_check
                
                # Just mount the existing EFI partition
                mkdir -p $INSTALL_ROOT/boot/efi
                mount -v $EFI_PARTITION $INSTALL_ROOT/boot/efi
                return
            else
                log "EFI partition is formatted but empty - will reformat"
                umount /tmp/efi_check
                rmdir /tmp/efi_check
            fi
        else
            log "Cannot mount EFI partition - will reformat"
            rmdir /tmp/efi_check 2>/dev/null || true
        fi
    else
        log "EFI partition is not formatted or not FAT32"
    fi
    
    # Format the EFI partition
    log "Formatting EFI partition as FAT32..."
    mkfs.fat -F32 -n EFI $EFI_PARTITION
    mkdir -p $INSTALL_ROOT/boot/efi
    mount -v $EFI_PARTITION $INSTALL_ROOT/boot/efi
}

format_boot() {
    log "Formatting boot partition..."
    if [[ "$PARTITION_MODE" == "auto" ]]; then
        mkfs.ext4 -F -F -L boot $BOOT_PARTITION
    else
        mkfs.ext4 -F -F -L boot $BOOT_PARTITION
    fi
    mkdir -p $INSTALL_ROOT/boot
    mount -v $BOOT_PARTITION $INSTALL_ROOT/boot
}

format_swap() {
    if [[ -n "$SWAP_PARTITION" ]]; then
        log "Formatting swap partition..."
        if [[ "$PARTITION_MODE" == "auto" ]]; then
            mkswap -f $SWAP_PARTITION
        else
            mkswap $SWAP_PARTITION
        fi
    else
        log "No swap partition configured - skipping swap formatting"
    fi
}

install_base_system() {
    log "Installing base Ubuntu system ($DEBOOTSTRAP_SUITE)..."
    debootstrap --cache-dir="$CACHE_DIR" $DEBOOTSTRAP_SUITE $INSTALL_ROOT
}

configure_system() {
    log "Configuring system..."
    
    # Set hostname
    echo "$HOSTNAME" > $INSTALL_ROOT/etc/hostname
    
    # Configure hosts
    cat > $INSTALL_ROOT/etc/hosts << EOF
127.0.0.1       localhost
127.0.1.1       $HOSTNAME

# The following lines are desirable for IPv6 capable hosts
::1     localhost ip6-localhost ip6-loopback
ff02::1 ip6-allnodes
ff02::2 ip6-allrouters
EOF
    
    # Copy network configuration
    log "Configuring network setup..."
    mkdir -p $INSTALL_ROOT/etc/netplan
    
    if [[ "$USE_NETWORK_MANAGER" == "true" ]]; then
        # Create minimal NetworkManager netplan configuration
        log "Creating NetworkManager netplan configuration"
        cat > $INSTALL_ROOT/etc/netplan/00-default.yaml << 'EOF'
network:
  version: 2
  renderer: NetworkManager
EOF
    elif [[ -n "$NETPLAN_FILE" ]]; then
        # Use custom netplan file
        log "Using custom netplan file: $NETPLAN_FILE"
        cp "$NETPLAN_FILE" "$INSTALL_ROOT/etc/netplan/"
    elif [[ -d "/etc/netplan" ]]; then
        # Copy existing netplan configuration
        log "Copying existing netplan configuration from /etc/netplan"
        cp -r /etc/netplan/* $INSTALL_ROOT/etc/netplan/ 2>/dev/null || true
    else
        log "No netplan configuration found, using default network setup"
    fi
    
    # Configure APT sources
    log "Configuring APT sources..."
    # Comment out all existing lines in sources.list
    if [[ -f "$INSTALL_ROOT/etc/apt/sources.list" ]]; then
        sed -i 's/^[^#]/#&/' $INSTALL_ROOT/etc/apt/sources.list
    fi
    
    # Copy sources.list.d files
    if [[ -d "/etc/apt/sources.list.d" ]]; then
        mkdir -p $INSTALL_ROOT/etc/apt/sources.list.d
        cp -r /etc/apt/sources.list.d/* $INSTALL_ROOT/etc/apt/sources.list.d/ 2>/dev/null || true
    fi
}

bind_mount() {
    local source_dir="$1"
    local target_dir="$2"
    local recursive="${3:-false}"
    
    if [[ "$recursive" == "true" ]]; then
        log "Recursively bind mounting $source_dir to $target_dir..."
    else
        log "Bind mounting $source_dir to $target_dir..."
    fi
    
    # Create target directory if it doesn't exist
    mkdir -p "$target_dir"
    
    # Bind mount the main directory
    mount -v --bind "$source_dir" "$target_dir"
    mount -v --make-private "$target_dir"
    
    # If recursive, find all mounted subdirectories under the source and bind mount them
    if [[ "$recursive" == "true" ]]; then
        local mounted_subdirs
        mounted_subdirs=$(mount | awk -v src="$source_dir" '$3 ~ "^" src "/" {print $3}' | sort)
        
        if [[ -n "$mounted_subdirs" ]]; then
            while IFS= read -r subdir; do
                if [[ -n "$subdir" ]]; then
                    local relative_path="${subdir#$source_dir}"
                    local target_subdir="$target_dir$relative_path"
                    
                    log "Bind mounting subdir: $subdir -> $target_subdir"
                    mkdir -p "$target_subdir"
                    mount -v --bind "$subdir" "$target_subdir"
                    mount -v --make-private "$target_subdir"
                fi
            done <<< "$mounted_subdirs"
        fi
    fi
}

chroot_setup() {
    log "Setting up chroot environment..."
    
    # Recursively bind mount necessary filesystems
    bind_mount "/dev" "$INSTALL_ROOT/dev" true
    bind_mount "/proc" "$INSTALL_ROOT/proc" true
    bind_mount "/sys" "$INSTALL_ROOT/sys" true
    
    # Create essential runtime directories
    mkdir -p $INSTALL_ROOT/run/lock
    
    # Copy apt cache directory for package downloads
    log "Copying apt cache directory..."
    mkdir -p "$INSTALL_ROOT/var/cache"
    rsync -a /var/cache/apt/ "$INSTALL_ROOT/var/cache/apt/"
    
    # Create resolv.conf with reliable nameservers
    log "Creating resolv.conf with reliable nameservers..."
    if [[ -f "$INSTALL_ROOT/etc/resolv.conf" ]] || [[ -L "$INSTALL_ROOT/etc/resolv.conf" ]]; then
        mv "$INSTALL_ROOT/etc/resolv.conf" "$INSTALL_ROOT/etc/resolv.conf.backup"
    fi
    cat > "$INSTALL_ROOT/etc/resolv.conf" << 'EOF'
# DNS configuration for chroot environment
nameserver 1.1.1.1
nameserver 8.8.8.8
EOF
}

main() {
    # Parse command line arguments
    local skip_confirmation="false"
    local ssh_param_used="false"
    local ssh_param_value=""
    local NETPLAN_FILE=""
    local USE_NETWORK_MANAGER="false"
    local partition_mode_set=""
    
    while [[ $# -gt 0 ]]; do
        case $1 in
            -y|--yes)
                skip_confirmation="true"
                shift
                ;;
            -D|--debug)
                DEBUG="true"
                shift
                ;;
            -h|--hostname)
                if [[ -n "$2" && "$2" != -* ]]; then
                    HOSTNAME="$2"
                    shift 2
                else
                    echo "Error: -h|--hostname requires a hostname argument"
                    exit 1
                fi
                ;;
            -d|--disk)
                if [[ -n "$2" && "$2" != -* ]]; then
                    DISK="$2"
                    shift 2
                else
                    echo "Error: -d|--disk requires a disk device argument"
                    exit 1
                fi
                ;;
            --ssh)
                if [[ "$ssh_param_used" == "true" ]]; then
                    echo "Error: --ssh and --nossh cannot be used together"
                    exit 1
                fi
                INSTALL_SSH="true"
                ssh_param_used="true"
                ssh_param_value="--ssh"
                shift
                ;;
            --nossh)
                if [[ "$ssh_param_used" == "true" ]]; then
                    echo "Error: --ssh and --nossh cannot be used together"
                    exit 1
                fi
                INSTALL_SSH="false"
                ssh_param_used="true"
                ssh_param_value="--nossh"
                shift
                ;;
            --auto)
                if [[ "$PARTITION_MODE" == "manual" && -n "$partition_mode_set" ]]; then
                    echo "Error: --auto and --manual cannot be used together"
                    exit 1
                fi
                PARTITION_MODE="auto"
                partition_mode_set="true"
                shift
                ;;
            --manual)
                if [[ "$PARTITION_MODE" == "auto" && -n "$partition_mode_set" ]]; then
                    echo "Error: --auto and --manual cannot be used together"
                    exit 1
                fi
                PARTITION_MODE="manual"
                partition_mode_set="true"
                shift
                ;;
            --yaml)
                if [[ "$USE_NETWORK_MANAGER" == "true" ]]; then
                    echo "Error: --yaml and --use-nm cannot be used together"
                    exit 1
                fi
                if [[ -n "$2" && "$2" != -* ]]; then
                    NETPLAN_FILE="$2"
                    shift 2
                else
                    echo "Error: --yaml requires a filename argument"
                    exit 1
                fi
                ;;
            --use-nm)
                if [[ -n "$NETPLAN_FILE" ]]; then
                    echo "Error: --yaml and --use-nm cannot be used together"
                    exit 1
                fi
                USE_NETWORK_MANAGER="true"
                shift
                ;;
            *)
                echo "Unknown parameter: $1"
                echo "Usage: $0 [-y|--yes] [-D|--debug] [-h|--hostname HOSTNAME] [-d|--disk DEVICE] [--auto|--manual] [--ssh|--nossh] [--yaml FILENAME|--use-nm]"
                echo "  -y, --yes              Skip confirmation prompts in auto mode"
                echo "  -D, --debug            Enable debug mode (pause before chroot)"
                echo "  -h, --hostname HOSTNAME Override hostname from config"
                echo "  -d, --disk DEVICE      Override disk device from config"
                echo "  --auto                 Use automatic partitioning (overrides config)"
                echo "  --manual               Use manual partitioning (overrides config)"
                echo "  --ssh                  Force SSH installation (overrides config)"
                echo "  --nossh                Skip SSH installation (overrides config)"
                echo "  --yaml FILENAME        Use custom netplan YAML file instead of default"
                echo "  --use-nm               Use NetworkManager renderer (conflicts with --yaml)"
                exit 1
                ;;
        esac
    done
    
    # Validate netplan file if provided
    if [[ -n "$NETPLAN_FILE" ]]; then
        if [[ ! -f "$NETPLAN_FILE" ]]; then
            error "Netplan file does not exist: $NETPLAN_FILE"
        fi
        
        # Check if file has .yaml or .yml extension
        if [[ ! "$NETPLAN_FILE" =~ \.(yaml|yml)$ ]]; then
            warn "Netplan file should have .yaml or .yml extension: $NETPLAN_FILE"
        fi
        
        log "Using custom netplan file: $NETPLAN_FILE"
    fi
    
    # Enable debug tracing if DEBUG is true (from config or command line)
    if [[ "${DEBUG:-false}" == "true" ]]; then
        set -x
    fi
    
    log "Starting Ubuntu 22.04 Root on ZFS installation - Stage 1..."
    
    check_root
    check_uefi_boot
    validate_required_config
    check_configuration "$skip_confirmation"
    install_zfs
    detect_existing_pools
    partition_disk
    create_zfs_pools
    create_datasets
    format_boot
    format_efi
    format_swap
    configure_dataset_mounting
    install_base_system
    configure_system
    chroot_setup
    
    # Copy stage2 script to installation root
    log "Copying stage2 script to installation root..."
    cp "$SCRIPT_DIR/ubuntu-stage2.sh" $INSTALL_ROOT/root/
    cp "$SCRIPT_DIR/ubuntu-config.sh" $INSTALL_ROOT/root/
    chmod +x $INSTALL_ROOT/root/ubuntu-stage2.sh
    
    log "Stage 1 complete."
    
    if [[ "$AUTO_RUN_STAGE2" == "true" ]]; then
        log "Automatically running Stage 2..."
        
        # Prepare stage2 command with debug and SSH parameters if needed
        local stage2_cmd="/root/ubuntu-stage2.sh"
        if [[ "$DEBUG" == "true" ]]; then
            stage2_cmd="$stage2_cmd -D"
        fi
        if [[ "$ssh_param_used" == "true" ]]; then
            stage2_cmd="$stage2_cmd $ssh_param_value"
        fi
        
        # Execute stage 2 with debug break handling
        local stage2_exit_code=0
        chroot $INSTALL_ROOT bash --login -c "$stage2_cmd" || stage2_exit_code=$?
        
        if [[ $stage2_exit_code -eq 99 ]]; then
            warn "DEBUG BREAK: Stage 2 debug break triggered"
            warn "Installation stopped for debugging purposes"
            exit 99
        elif [[ $stage2_exit_code -eq 0 ]]; then
            log "Stage 2 completed successfully."
            log "Automatically running Stage 3..."
            
            # Prepare stage3 command with debug parameter if needed
            local stage3_cmd="$SCRIPT_DIR/ubuntu-stage3.sh"
            if [[ "$DEBUG" == "true" ]]; then
                stage3_cmd="$stage3_cmd -D"
            fi
            
            # Execute stage 3 with debug break handling
            local stage3_exit_code=0
            eval "$stage3_cmd" || stage3_exit_code=$?
            
            if [[ $stage3_exit_code -eq 99 ]]; then
                warn "DEBUG BREAK: Stage 3 debug break triggered"
                warn "Installation stopped for debugging purposes"
                exit 99
            elif [[ $stage3_exit_code -eq 0 ]]; then
                log "Stage 3 completed successfully."
                log "Installation complete! System is ready for reboot."
                warn "Please remove the installation media and reboot:"
                warn "sudo reboot"
            else
                error "Stage 3 failed. You can run it manually:"
                log "./ubuntu-stage3.sh"
            fi
        else
            error "Stage 2 failed. You can run it manually:"
            log "chroot $INSTALL_ROOT /usr/bin/env bash -l"
            log "Then run: ./ubuntu-stage2.sh"
        fi
    else
        log "To continue manually:"
        log "chroot $INSTALL_ROOT /usr/bin/env bash -l"
        log "Then run: ./ubuntu-stage2.sh"
    fi
}

# Run main function
main "$@"